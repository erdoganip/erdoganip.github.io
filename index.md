<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TLK47QPQQP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TLK47QPQQP');
</script>

## Portfolio

---

### Pattern Recognition
[Expectation Maximization](/expectation_maximization)
<br/>
In this project, I implemented Expectation Maximization algorithm as a solution to cluster a dataset which is a Gaussian Mixture model, includes three different Gaussian distribution.
<br/>
<img src="images/pr_hw2_final.png" width="50%" height="50%"/>

---
[Logistic Regression and Gradient Descent from Scratch](/log_res)
<br/>
This project is about implementing a logistic regression model by scratch and updating its gradients using stochastic gradient descent method.
<br/>
<img src="images/pr3_hw_accuracies.jpg" width="50%" height="50%"/>

---
[Project 3 Title](http://example.com/)
<img src="images/dummy_thumbnail.jpg?raw=true"/>

---

### Deep Learning

[MLP as a Neural Language Model](/mlp_language)
<br/>
In this project, I implemented a neural language model using a multi-layer perceptron. This network receives 3 consecutive words as the input and predicts the next word.
<br/>
<img src="images/mlp.png" width="50%" height="50%"/>

[CNN from Scratch](/cnn_from_scratch)
<br/>
I implemented a convolutional neural network (CNN) architecture from scratch, using Pytorch. Tried different data augmentation and optimization techniques and to boost the performance on CIFAR10 dataset.
<br/>
<img src="images/model9.png" width="25%" height="25%"/>
<img src="images/cifar10.png" width="75%" height="75%"/>
<br/>
Source for CIFAR10 dataset examples: https://www.cs.toronto.edu/~kriz/cifar.html

- [Project 3 Title](http://example.com/)
- [Project 4 Title](http://example.com/)
- [Project 5 Title](http://example.com/)

---




---
<p style="font-size:11px">Page template forked from <a href="https://github.com/evanca/quick-portfolio">evanca</a></p>
<!-- Remove above link if you don't want to attibute -->
